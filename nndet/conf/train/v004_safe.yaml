# @package __global__
defaults:
  - augmentation: base_more

module: RetinaUNetV004
predictor: BoxPredictorSelective

plan: D3V001_3d
planner: D3V001

augment_cfg:
  augmentation: ${augmentation}
  num_train_batches_per_epoch: ${trainer_cfg.num_train_batches_per_epoch}
  num_val_batches_per_epoch: ${trainer_cfg.num_val_batches_per_epoch}

  dataloader: "DataLoader{}DOffset"
  oversample_foreground_percent: 0.5
  dataloader_kwargs: {}

  # 安全的数据加载配置
  num_threads: ${oc.env:det_num_threads, "12"}  # 减少线程数避免崩溃
  num_cached_per_thread: 2  # 保持较小的缓存
  multiprocessing: True

  # 安全的batch size（先从6开始测试）
  batch_size: 6

trainer_cfg:
  gpus: 1
  accelerator:
  precision: 16
  amp_backend: native
  amp_level: O1
  
  # 性能优化但保持稳定
  deterministic: False
  benchmark: True
  
  monitor_key: "mAP_IoU_0.10_0.50_0.05_MaxDet_100"
  monitor_mode: "max"

  max_num_epochs: 50
  num_train_batches_per_epoch: 2200  # 调整批次数
  num_val_batches_per_epoch: 90

  # 适应batch size的学习率
  initial_lr: 0.015  # 适中的学习率
  sgd_momentum: 0.9
  sgd_nesterov: True
  weight_decay: 3.e-5

  warm_iterations: 3500
  warm_lr: 1.e-6

  poly_gamma: 0.9
  swa_epochs: 10

model_cfg:
  encoder_kwargs: {}
  decoder_kwargs:
    min_out_channels: 8
    upsampling_mode: "transpose"
    num_lateral: 1
    norm_lateral: False
    activation_lateral: False
    num_out: 1
    norm_out: False
    activation_out: False

  head_kwargs: {}

  head_classifier_kwargs:
    num_convs: 1
    norm_channels_per_group: 16
    norm_affine: True
    reduction: "mean"
    loss_weight: 1.
    prior_prob: 0.01

  head_regressor_kwargs:
    num_convs: 1
    norm_channels_per_group: 16
    norm_affine: True
    reduction: "sum"
    loss_weight: 1.
    learn_scale: True

  head_sampler_kwargs:
    batch_size_per_image: 40  # 适中的采样数
    positive_fraction: 0.33
    pool_size: 20
    min_neg: 1

  segmenter_kwargs:
    dice_kwargs:
      batch_dice: True

  matcher_kwargs:
    num_candidates: 4
    center_in_gt: False

  vessel_attention_kwargs:
    enable: True
    fusion_mode: "concatenation"
    attention_channels: 8
    use_sigmoid: True
    spatial_scale: 1.0

  plan_arch_overwrites: 
    batch_size: 6  # 安全的batch size
  plan_anchors_overwrites: {} 